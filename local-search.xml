<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>搜广推学习之路</title>
    <link href="/Post1/"/>
    <url>/Post1/</url>
    
    <content type="html"><![CDATA[<!-- 添加音乐名片 https://github.com/metowolf/MetingJS --><!-- require APlayer --><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><!-- require MetingJS --><script src="https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js"></script><p><meting-js    server="netease"    type="song"    id="2707655756"> </meting-js></p><p>目前研0，本科为隐私保护方向，由于安全岗位的稀缺以及并不打算继续读博，在各方面考虑下想要尝试零基础转搜广推，争取在年底前找到一份日常实习，在此记录我的学习过程，夯实基础的同时为后来者提供经验。</p><p>这是我在网上找到的<a href="https://www.xiaohongshu.com/explore/687339c9000000001203212f?xsec_token=CBW_s-Z8Fc-se6uDqatKGzEqwLxXtV08CZ3k1WsmtQnRk=&amp;xsec_source=app_share">参考路线</a>。</p><h2 id="机器学习">机器学习</h2><p>快速：基础概念、树和集成学习等（尤其是逻辑回归、bagging、boosting部分） 长期：吴恩达机器学习</p><h3 id="绪论">绪论</h3><h4 id="基本术语">基本术语</h4><p>首先假定我们收集了一批关于西瓜的数据，例如（色泽=青绿；根蒂=蜷缩；敲声=浊响），（色泽=乌黑；根蒂=稍蜷；敲声=沉闷），（色泽=浅白；根蒂=硬挺；敲声=清脆）</p><ul><li>数据集：这组关于西瓜数据的集合即为数据集</li><li>样本（示例）：每条记录是关于每个西瓜的记录，称为一个样本（示例）</li><li>属性（特征）：色泽、根蒂、敲声描述西瓜的性质，称为属性（性质）</li><li>属性值（特征值）：青绿、乌黑等为属性上的取值，称为属性值（特征值）</li><li>属性空间（样本空间、输入空间）：将色泽、根蒂、敲声作为坐标轴，可以形成一个描述西瓜的<strong>三维</strong>空间（d个属性即为d维），称为属性空间（样本空间、输入空间）</li><li>特征向量：每个西瓜在属性空间中有自己的坐标位置，对应一个坐标向量，所以一个样本也称为一个特征向量</li><li>训练（学习）：从数据中学得模型的过程称为<strong>学习或训练</strong>，训练过程中使用的数据称为<strong>训练数据</strong>，使用的每个样本为一个<strong>训练样本</strong>，这些训练样本的集合为<strong>训练集</strong></li><li>学得模型对应了数据中的某一潜在规律，故又称为<strong>假设</strong>，该潜在规律自身，称为<strong>真实或真相</strong>，机器学习的过程就是要令假设接近真相</li></ul><p>在上述概念基础上，如果我们想要学得的模型是一个可判断西瓜是否为好瓜的模型，那么我们应在训练样本（色泽=青绿；根蒂=蜷缩；敲声=浊响）的基础上添加“结果”信息——好瓜，如（（色泽=青绿；根蒂=蜷缩；敲声=浊响），好瓜）</p><ul><li>标记（label）:关于示例结果的信息（好瓜 or 坏瓜）即为标记</li><li>样例：有label的示例即为样例</li><li>标记空间（输出空间）：所有label的集合 根据预测结果的不同，我们可以将学习任务分为两大类：</li><li>分类：若预测的是离散值，例如好瓜、坏瓜，此类学习任务称为分类(classification)，只有两个类别的二分类任务，通常成其中一个类别为“正类”，另一个类别为“反类”</li><li>回归：若预测的是连续值，例如西瓜成熟度0.97、0.35，此类学习任务称为回归(regression)</li><li>测试：学得模型后，用其进行预测的过程称为测试，被预测的样本称为测试样本</li></ul><p>除此之外，即使无label，我们也可以对西瓜进行聚类，将训练集中的西瓜按照其属性分为若干组，每组称为一个“簇”。例如根据色泽，可将西瓜分为深色瓜和浅色瓜等</p><p>根据训练数据是否有label，可以将学习任务大致划分为两大类<strong>监督学习</strong>和<strong>无监督学习</strong></p><ul><li>监督学习：有label，分类和回归是监督学习的代表</li><li>无监督学习：无label，聚类是无监督学习的代表</li></ul><p>机器学习的目标是通过训练数据，使模型也适用于陌生数据，所以模型适用于新样本的能力就称为<strong>泛化能力</strong></p><h4 id="假设空间">假设空间</h4><p>假设空间是由输入空间到输出空间的映射的集合。 假设空间是所有假设的集合，学习过程可以看作是在假设空间中进行搜索的过程，目的是找到与训练集匹配的假设。而现实中，由于训练集有限，所以可能会有多个假设与训练集一致，即存在一个与训练集一致的“假设集合”，称之为“版本空间”。</p><ul><li>具体的例子，比如针对西瓜是否为好瓜这一问题，一个假设就是一个判断规则，比如“如果色泽=青绿，根蒂=蜷缩，那么就是好瓜”或“如果根蒂=硬挺，那么是坏瓜”等，这都是假设，而所有假设构成了假设空间。</li></ul><h4 id="归纳偏好">归纳偏好</h4><p>在面对新样本时，模型采用版本空间中的哪一假设？这事就涉及到了采用算法的归纳偏好，不同的算法有不同的偏好。需要注意，<strong>任何一个机器学习算法都必有其归纳偏好，否则将无法产生确定的学习结果。</strong> 同时对于算法A来说，若其在某些方面比算法B好，那么必然存在一些方面B比A好。这个结论对任何算法均成立，无一例外！所以脱离实际谈论哪个算法更好是毫无意义的。</p><h3 id="模型评估与选择">模型评估与选择</h3><h4 id="经验误差与过拟合">经验误差与过拟合</h4><ul><li>错误率：分类错误的样本数占样本总数的比例</li><li>精度：1-错误率，</li></ul><p>如果在m个样本中有a个样本分类错误，那么错误率 E = a/m，精度 = 1 - E。</p><ul><li>误差：更一般地，把学习器的实际预测输出与样本的真实输出之间的差异称为误差。学习器在训练集上的误差称为<strong>训练误差或经验误差</strong>，在新样本上的误差称为<strong>泛化误差</strong>。</li><li>过拟合：学习能力过强，把训练数据中的非普遍性特征甚至随机噪声都当成了普遍的规律来学习。</li><li>欠拟合：学习能力低下 <img src="/img/过拟合.png" alt="过拟合、欠拟合的直观类比" /></li></ul><h4 id="评估方法">评估方法</h4><p>我们在选择模型时希望选择泛化误差小的模型，因此我们需要一个测试集来测试学习器对于新样本的判别能力，然后使用测试误差作为泛化误差的近似。注意，<strong>训练集和测试集要尽量互斥。</strong> 我们拥有一个数据集D（大小为m），通过以下几种方法对数据集D进行处理，从中产生训练集S和测试集T：</p><ul><li>留出法：直接将数据集D划分为两个<strong>互斥</strong>的集合，其中一个作为训练集S，另一个作为测试集T，一般将约2/3 ~ 4/5的样本用于训练，剩余样本用于测试。且由于单次留出法的估计结果往往不够稳定可靠，使用留出法时一般采用若干次随机划分、重复进行试验评估后取平均值作为留出法的评估结果。</li><li>交叉验证法：将数据集D划分为k个大小相似的互斥子集，然后每次用k-1个子集的并集作为训练集，剩下的那个子集作为测试集，这样就可以得到k组训练、测试集，从而进行k次训练和测试，最终返回这k次测试的均值。 <img src="/img/交叉验证法.png" alt="10折交叉验证法" /></li><li>自助法：从数据集D中<strong>随机有放回</strong>地抽取m次样本，得到一个新的数据集（大小也是m），这个新的数据集可能包含重复的样本。使用这个新的数据集来训练模型，剩下的没有被抽取的样本作为测试集。（根据数学计算，大概有1/3的数据可用于测试）</li></ul><h4 id="性能度量">性能度量</h4><ul><li>均方误差：在<strong>回归</strong>任务中，最常用的指标就是均方误差。 <img src="/img/均方误差.png" alt="公式" /></li></ul><p>接下来介绍分类任务中常用的性能指标</p><ul><li>错误率与精度：错误率是<strong>分类错误</strong>的样本数占样本总数的比例，精度是<strong>分类正确</strong>的样本数占样本总数的比例。</li><li>查准率(precision)、查全率(recall)与F1：查准率也叫做准确率，查全率也叫做召回率。对于二分类问题，根据分类器在测试集上的预测正确与否，将样本划分为四类：</li></ul><ol type="1"><li>TP (True Positive)：真正例，正确预测为正类的样本数量。</li><li>TN (True Negative)：真负例，正确预测为负类的样本数量。</li><li>FP (False Positive)：假正例，错误预测为正类的样本数量。</li><li>FN (False Negative)：假负例，错误预测为负类的样本数量。 查准率P与查全率R定义如下： <img src="/img/PR.png" alt="查准率、查全率" /></li></ol><p>由混淆矩阵易得，查准率指模型预测为正类的样本中，实际为正类的比例，查全率指实际为正类的样本中，被模型正确预测为正类的比例。 一般来说，查全率和查准率是一对矛盾的度量，我们将查准率为纵轴，查全率为横轴作图，就可以得到查准率-查全率曲线，简称“P-R曲线”。如图，A将C完全“包住”，所以A的性能显然优于C，但对于A和B来讲，两者有交叉，无法直观比较两者的优劣，这时会采用平衡点（BEP）来度量，认为A优于B，在平衡点的基础上，进一步优化，得到更常用的<strong>F1度量</strong>。 <img src="/img/P-R曲线.png" alt="P-R曲线" /> <img src="/img/F1度量.png" alt="F1度量公式" /></p><h3 id="线性回归">线性回归</h3><p>假设有n个样本，每个样本有d个属性，例如 <span class="math inline"><em>x</em><sup>(<em>i</em>)</sup> = (<em>x</em><sub>1</sub><sup>(<em>i</em>)</sup>; <em>x</em><sub>2</sub><sup>(<em>i</em>)</sup>; ...; <em>x</em><sub><em>d</em></sub><sup>(<em>i</em>)</sup>)<sup><em>T</em></sup></span>，其中<span class="math inline"><em>x</em><sub><em>j</em></sub><sup>(<em>i</em>)</sup></span>是<span class="math inline"><em>x</em><sup>(<em>i</em>)</sup></span>在第j个属性上的取值，线性模型希望学得预测函数 <span class="math inline"><em>ŷ</em><sup>(<em>i</em>)</sup> = <em>w</em><sub>1</sub><em>x</em><sub>1</sub><sup>(<em>i</em>)</sup> + <em>w</em><sub>2</sub><em>x</em><sub>2</sub><sup>(<em>i</em>)</sup> + ... + <em>w</em><sub><em>d</em></sub><em>x</em><sub><em>d</em></sub><sup>(<em>i</em>)</sup> + <em>b</em></span>（使用<span class="math inline"><em>ŷ</em><sup>(<em>i</em>)</sup></span>表示<span class="math inline"><em>y</em><sup>(<em>i</em>)</sup></span>的预测值），一般写作向量形式 <span class="math inline"><em>ŷ</em><sup>(<em>i</em>)</sup> = <em>w</em><sup><em>T</em></sup><em>x</em><sup>(<em>i</em>)</sup> + <em>b</em></span>，其中<span class="math inline"><em>w</em> = (<em>w</em><sub>1</sub>; <em>w</em><sub>2</sub>; ...; <em>w</em><sub><em>d</em></sub>)<sup><em>T</em></sup></span>，当w和b确定时，线性模型也就确定了。 由于实际情况中得到<span class="math inline"><em>ŷ</em><sup>(<em>i</em>)</sup> = <em>y</em><sup>(<em>i</em>)</sup></span>是很难的，所以我们需要一个度量指标来判断模型对数据的拟合程度。<strong>损失函数</strong>就可以量化目标的实际值与预测值之间的差距。通常我们使用非负数来作为损失，数值越小损失越小拟合程度越好，当完美预测时损失值为0。回归问题中最常用的损失函数为平方误差函数，对于样本i，预测值为 <span class="math inline"><em>ŷ</em><sup>(<em>i</em>)</sup></span>，真实标签值为 <span class="math inline"><em>y</em><sup>(<em>i</em>)</sup></span>，那么平方误差SE就表示为 <img src="/img/QianJianTec1761797236022.svg" alt="损失函数" /></p><p>（<strong>注意：</strong>这里的1/2不会带来本质差别，其存在的意义是之后对损失函数求导后可令系数为1，在形式上简单一些） 上述为度量一个样本，为度量模型在整个数据集上的质量，我们需要计算训练集在n个样本上的损失均值，即均方误差MSE为： <img src="/img/损失均值.png" alt="损失函数" /> 有了度量指标，那么我们的目标就是找到w和b使得训练集上的总损失最小： <img src="/img/损失最小化.png" alt="损失最小化" /></p>接下来为实现损失最小化，首先，我们将偏置b合并到参数w中，令<span class="math inline"><em>b</em> = <em>w</em><sub><em>d</em> + 1</sub></span>,合并方法是在包含所有参数的矩阵中附加一列，如图所示。 首先为参数的初始状态： <!-- ![初始](/img/QianJianTec1761919161520.jpg) --><center><img src="/img/QianJianTec1761919161520.jpg" width="75%" height="75%" /></center>我们将b合并到w中： <!-- ![合并偏置项](/img/QianJianTec1761919084433.jpg) --><center><img src="/img/QianJianTec1761919084433.jpg" width="75%" height="75%" /></center>我们将所有样本合并起来： <!-- ![合并样本](/img/QianJianTec1761918784006.png) --><center><img src="/img/QianJianTec1761918784006.png" width="75%" height="75%" /></center>令<span class="math inline"><em>X</em> = <em>x</em><sup><em>T</em></sup></span>： <!-- ![](/img/QianJianTec1761918118840.jpg) --><center><img src="/img/QianJianTec1761918118840.jpg" width="75%" height="75%" /></center>所以均方误差可转换为下列形式： <!-- ![均方误差](/img/QianJianTec1761922200942.jpg) --><center><img src="/img/QianJianTec1761922200942.jpg" width="40%" height="40%" /></center><p>因此我们的目标就是最小化<span class="math inline">||<em>X</em><em>w</em> − <em>y</em>||<sup>2</sup></span>，我们令MSE关于w的导数为0可以得到解析解，（正规方程，这里不再赘述，感兴趣的可以自行了解），但实际情况不是所有问题都有解析解存在，所以我们采用<strong>梯度下降</strong>的方法。</p><blockquote><p><strong>梯度下降的思想：</strong>开始时随机选择w和b，计算损失函数，然后寻找下一个能让损失函数减少最多的参数组合（当前参数的梯度（即偏导数的向量）的反方向，梯度指明了损失函数最陡峭上升的方向。），持续这么做可以找到一个局部最小值，但这个值不一定为全局最小值，所以采取不同的初始参数组合，可能会得到不同的局部最小值。</p></blockquote>随机梯度下降（SGD）：在一次SGD更新中，随机选取一个训练样本<span class="math inline">(<em>x</em><sup>(<em>i</em>)</sup>, <em>y</em><sup>(<em>i</em>)</sup>)</span>用这个样本计算w的梯度并更新。 <!-- ![SGD](/img/QianJianTec1761928721457.jpg) --><center><img src="/img/QianJianTec1761928721457.jpg" width="40%" height="40%" /></center><p>其中，<span class="math inline"><em>α</em></span>为学习率，决定了每次更新的步长。 <img src="/img/学习率大小的影响.png" alt="学习率大小的影响" /></p>在我们实际应用中，我们会将所有样本都遍历一遍来得到最后的w，这就导致执行速度很慢，因此产生了变体<strong>小批量随机梯度下降</strong>，我们在每次更新时随机抽取一小批样本<span class="math inline">ℬ</span>,它是由固定数量的训练样本组成的。 然后，我们计算小批量的平均损失关于模型参数的导数（也可以称为梯度）。 <!-- ![小批量SGD](/img/QianJianTec1761930267754.jpg) --><center><img src="/img/QianJianTec1761930267754.jpg" width="45%" height="45%" /></center><h3 id="逻辑回归">逻辑回归</h3><p>逻辑回归是一种广泛应用于分类问题的统计学习方法，尽管名字中带有“回归”，但它实际上是一种用于<strong>二分类或多分类</strong>问题的算法。 逻辑回归通过使用逻辑函数（也称为 <strong>Sigmoid 函数</strong>）将线性回归的输出映射到 0 和 1 之间，从而预测某个事件发生的概率。 逻辑回归广泛应用于各种分类问题，例如：</p><ul><li>垃圾邮件检测（是/不是）</li><li>肿瘤预测（恶性/良性）</li></ul>sigmoid函数是一个S形函数： <!-- ![sigmoid函数](/img/sigmoid函数.png) --><center><img src="/img/sigmoid函数.png" width="35%" height="35%" /></center><p><br /></p><img src="/img/sigmoid图像.png" alt="sigmoid图像" /> 在逻辑回归中，其输入形式上等同于线性回归的输出，即: <!-- ![逻辑回归公式](/img/QianJianTec1761973408418.png) --><center><img src="/img/QianJianTec1761973408418.png" width="75%" height="75%" /></center><p>在逻辑回归中，我们将正类标记为1，反类标记为0。<span class="math inline"><em>h</em><sub><em>w</em></sub>(<em>x</em>)</span>函数的作用就是给定输入变量，计算出输出等于1的可能性，即<span class="math inline"><em>h</em><sub><em>w</em></sub>(<em>x</em>) = <em>P</em>(<em>y</em> = 1|<em>x</em>; <em>w</em>)</span>。</p><ul><li>例如对一个肿瘤的样本，计算得到 h_w(x) = 0.7，也就是该肿瘤有70%的可能是恶性的。</li></ul><p>一般我们会设置阈值为0.5，当<span class="math inline"><em>h</em><sub><em>w</em></sub>(<em>x</em>) ≥ 0.5</span>时，预测y=1；当<span class="math inline"><em>h</em><sub><em>w</em></sub>(<em>x</em>) &lt; 0.5</span>时，预测y=0。</p><ul><li>例如：假设现在有一个模型，参数w是向量[-3 1 1]。则当 -3+x1+x2 <span class="math inline">≥</span> 0 ，即 x1+x2 <span class="math inline">≥</span> 3 时，模型将预测 y=1 。我们可以绘制直线 x1+x2=3 ，这条线便是我们模型的分界线，将预测为1的区域和预测为0的区域分隔开。 如下图： <img src="/img/例子.png" /></li></ul><p>同线性回归，为找到合适的w，我们需要定义逻辑回归的损失函数，如果我们沿用线性回归中的均方误差为损失函数，将<span class="math inline"><em>h</em><sub><em>w</em></sub>(<em>x</em>)</span>带入其中，会发现我们得到的损失函数是一个非凸函数，如图所示： <img src="/img/非凸函数.png" /> 在这种情况下，损失函数存在许多局部最小值，会影响梯度下降法寻找全局最小值，所以我们不能使用均方误差作为逻辑回归的损失函数，我们重新定义损失函数。</p><p>首先，我要先介绍一下<strong>极大似然估计</strong></p><blockquote><p>极大似然估计是一种在统计学中用于估计概率模型参数的方法。其基本思想是：给定一组数据和一个概率模型，最大似然估计会找到模型参数的值，使得这组数据在该模型下出现的概率（即“似然性”）最大。</p></blockquote><p>已知模型参数<span class="math inline">(<em>θ</em><sub>1</sub>, <em>θ</em><sub>2</sub>, ..., <em>θ</em><sub><em>k</em></sub>)</span>，以及数据集D=<span class="math inline"><em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, ..., <em>x</em><sub><em>n</em></sub></span>，极大似然估计就是要找到<span class="math inline"><em>θ̂</em></span>使得数据集D出现的概率最大，所以定义似然函数： <img src="/img/似然函数.png" alt="似然函数" /></p><p>然后对似然函数求导，令导数为0得到的<span class="math inline"><em>θ̂</em></span>即为所求，为方便计算，我们一般对等式两侧取对数然后再计算。</p>在我们的逻辑回归中，存在下列关系： <!-- ![](/img/Snipaste_2025-11-01_16-25-15.png) --><center><img src="/img/Snipaste_2025-11-01_16-25-15.png" width="50%" height="50%" /></center>综合起来可写成： <!-- ![](/img/Snipaste_2025-11-01_16-32-57.png) --><center><img src="/img/Snipaste_2025-11-01_16-32-57.png" width="75%" height="75%" /></center><p>由极大似然估计的原理可写出单个样本i的对数似然函数： <img src="/img/Snipaste_2025-11-01_16-51-12.png" /> 最大化对数似然函数就可以求得最符合数据的模型参数w，而我们的损失函数是要最小化，所以我们在对数似然函数前加个负号就可以作为损失函数，同时我们使用整个数据集，得到逻辑回归的损失函数函数： <img src="/img/Snipaste_2025-11-01_16-57-40.png" /></p><p>我们的目标就是最小化J(w)，具体方法也可采用梯度下降，具体推导不再赘述，与线性回归类似。</p><h3 id="决策树">决策树</h3><h2 id="深度学习">深度学习</h2><p>前16小节 + 注意力机制（李沐动手学深度学习）</p><h2 id="推荐系统课程">推荐系统课程</h2><p>王树森推荐系统课程</p><h2 id="leetcode-hot-100">LeetCode hot 100</h2><h2 id="相关项目">相关项目</h2><p>阿里云天池 or kaggle 找一个推荐算法的项目</p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
